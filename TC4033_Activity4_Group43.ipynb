{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosiasRuiz/AdvanceML/blob/main/TC4033_Activity4_Group43.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037e89c8",
      "metadata": {
        "id": "037e89c8"
      },
      "source": [
        "## TC 5033\n",
        "### Text Generation\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
        "<br>\n",
        "\n",
        "- Objective:\n",
        "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
        "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
        "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Instructions:\n",
        "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
        "\n",
        "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
        "\n",
        "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation.\n",
        "\n",
        "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
        "\n",
        "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
        "\n",
        "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Evaluation Criteria:\n",
        "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
        "\n",
        "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
        "\n",
        "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function.\n",
        "\n",
        "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group 43\n",
        "\n",
        "* Andrea M. Ruiz G. - A01794631\n",
        "* Josías Ruiz P. - A00968460\n",
        "* Saúl Y. Salgueiro L. - A0XXXXXXX\n",
        "* Jesús Á. Salazar M. - A00513236"
      ],
      "metadata": {
        "id": "Xk7w_V9DVCDI"
      },
      "id": "Xk7w_V9DVCDI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalación de la paqueteria necesaria."
      ],
      "metadata": {
        "id": "_sPpJjO5QrK1"
      },
      "id": "_sPpJjO5QrK1"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'portalocker>=2.0.0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thMOwDCauHCe",
        "outputId": "54b908d6-90aa-437a-8ef6-746dc2806bf6"
      },
      "id": "thMOwDCauHCe",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importar librerias necesarias"
      ],
      "metadata": {
        "id": "mO2mUQfMQ0G9"
      },
      "id": "mO2mUQfMQ0G9"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3eb4b117",
      "metadata": {
        "id": "3eb4b117"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#PyTorch libraries\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import WikiText2\n",
        "# Dataloader library\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "# Libraries to prepare the data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# neural layers\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuracion para usar GPU"
      ],
      "metadata": {
        "id": "1qabpmGBQ8Ya"
      },
      "id": "1qabpmGBQ8Ya"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6d8ff971",
      "metadata": {
        "id": "6d8ff971"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Lz4ylrqHRmxf",
        "outputId": "dae2f5e9-4527-40c5-de1d-eedc42bab645"
      },
      "id": "Lz4ylrqHRmxf",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separacion de dataset en entrenamiento, validacion y prueba."
      ],
      "metadata": {
        "id": "Kzob3S6aRAYE"
      },
      "id": "Kzob3S6aRAYE"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f3288ce5",
      "metadata": {
        "id": "f3288ce5"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset, test_dataset = WikiText2()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se importa el diccionario de los tokens en ingles."
      ],
      "metadata": {
        "id": "aNeqLiTpRGLt"
      },
      "id": "aNeqLiTpRGLt"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fc4c7dbd",
      "metadata": {
        "id": "fc4c7dbd"
      },
      "outputs": [],
      "source": [
        "tokeniser = get_tokenizer('basic_english')\n",
        "def yield_tokens(data):\n",
        "    for text in data:\n",
        "        yield tokeniser(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Del data set de entrenamiento se define el vocabulario considerando la omisión de caracteres especiales."
      ],
      "metadata": {
        "id": "NS1OldVoROjR"
      },
      "id": "NS1OldVoROjR"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2c2cb068",
      "metadata": {
        "id": "2c2cb068"
      },
      "outputs": [],
      "source": [
        "# Build the vocabulary\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "#set unknown token at position 0\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con el dataset se preprocesan los datos de texto para el modelo de lenguaje. La función data_process toma una lista de cadenas de texto sin procesar y una longitud de secuencia como entrada. Luego, tokeniza el texto y convierte los tokens en tensores. Finalmente, elimina los tensores vacíos y crea tensores para los conjuntos de entrenamiento, validación y prueba."
      ],
      "metadata": {
        "id": "BFFwo0BxRb_O"
      },
      "id": "BFFwo0BxRb_O"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "134b832b",
      "metadata": {
        "id": "134b832b"
      },
      "outputs": [],
      "source": [
        "seq_length = 50\n",
        "def data_process(raw_text_iter, seq_length = 50):\n",
        "  data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "  data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) #remove empty tensors\n",
        "  return (data[:-(data.size(0)%seq_length)].view(-1, seq_length),\n",
        "          data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))\n",
        "# # Create tensors for the training set\n",
        "x_train, y_train = data_process(train_dataset, seq_length)\n",
        "x_val, y_val = data_process(val_dataset, seq_length)\n",
        "x_test, y_test = data_process(test_dataset, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxTX9FLNR0jI",
        "outputId": "78b5309e-1235-4d17-e2de-f617de9094c4"
      },
      "id": "rxTX9FLNR0jI",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40999, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5oyzYKCR3Ru",
        "outputId": "226a14e8-f462-4543-f3ad-24495c877772"
      },
      "id": "K5oyzYKCR3Ru",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40999, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformacion a forma de Tensor para los modelos."
      ],
      "metadata": {
        "id": "yknMmi9JSUOh"
      },
      "id": "yknMmi9JSUOh"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4b54c04d",
      "metadata": {
        "id": "4b54c04d"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "test_dataset = TensorDataset(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se utilizan Data loades para un eficiente carga de la informacion tambien sirve para iterar a traves de los batch para el entrenamiento."
      ],
      "metadata": {
        "id": "EjoDP6P1Sqsg"
      },
      "id": "EjoDP6P1Sqsg"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f4d400fb",
      "metadata": {
        "id": "f4d400fb"
      },
      "outputs": [],
      "source": [
        "batch_size = 64  # choose a batch size that fits your computation resources\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos el modelo LSTM que esta basado en una red neuronal completamente conectada. Este modelo va a predecir la siguente palabra en la secuencia."
      ],
      "metadata": {
        "id": "91WuSotKTGF8"
      },
      "id": "91WuSotKTGF8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**def forward(self, text, hidden)**: Este método define el pase adelante del modelo. Toma dos argumentos:\n",
        "  text: Un tensor de índices de palabras que representa la secuencia de entrada.\n",
        "  hidden: Una tupla de tensores de estado oculto que representa el estado oculto inicial de la capa LSTM.\n",
        "\n",
        "**embeddings = self.embeddings(text):** Esta línea aplica la capa de incrustación a la secuencia de entrada, convirtiendo cada índice de palabra en una representación vectorial.\n",
        "\n",
        "**output, hidden = self.lstm(embeddings, hidden):** Esta línea aplica la capa LSTM a la secuencia de entrada incrustada, produciendo un tensor de salida y actualizando el estado oculto.\n",
        "\n",
        "**decoded = self.fc(output):** Esta línea aplica la capa lineal totalmente conectada a la salida de la capa LSTM, produciendo un tensor de logaritmos sobre el vocabulario.\n",
        "\n",
        "**def init_hidden(self, batch_size):** Este método inicializa el estado oculto para la capa LSTM. Toma un argumento:\n",
        "  batch_size El tamaño del lote.\n",
        "\n",
        "**return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),:** Esta línea crea un tensor cero para el estado oculto de cada capa en el lote y lo convierte al dispositivo especificado (por ejemplo, CPU o GPU).\n",
        "\n",
        "**torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)):** Esta línea crea un tensor cero para el estado de la celda de cada capa en el lote y lo convierte al dispositivo especificado.\n"
      ],
      "metadata": {
        "id": "DjK-TqkJT6MT"
      },
      "id": "DjK-TqkJT6MT"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "59c63b01",
      "metadata": {
        "id": "59c63b01"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM model\n",
        "# Feel free to experiment\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, text, hidden):\n",
        "        embeddings = self.embeddings(text)\n",
        "        output, hidden = self.lstm(embeddings, hidden)\n",
        "        decoded = self.fc(output)\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = len(vocab) # vocabulary size\n",
        "emb_size = 100 # embedding size\n",
        "neurons = 128 # the dimension of the feedforward network model, i.e. # of neurons\n",
        "num_layers = 1 # the number of nn.LSTM layers\n",
        "model = LSTMModel(vocab_size, emb_size, neurons, num_layers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con el modelo definido se procede a estructurar la fase de entrenamiento, considerando las epocas y el optimizador."
      ],
      "metadata": {
        "id": "cTs1WPKMTqvT"
      },
      "id": "cTs1WPKMTqvT"
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, epochs, optimiser):\n",
        "  model = model.to(device=device)                                         # loads model into the given device\n",
        "  model.train()                                                           # sets the model in training model\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    (h, c) = model.init_hidden(batch_size)                                # initialize hidden state\n",
        "    for i, (data, targets) in enumerate((train_loader)):\n",
        "      h = h.detach()                                                      # detaches tensor from current graph (result will no require gradient)\n",
        "      c = c.detach()\n",
        "\n",
        "      inputs, targets = data.to(device), targets.to(device)               # loads x and y into the given device\n",
        "      outputs, (h, c) = model(inputs, (h, c))                             # performs forward pass\n",
        "\n",
        "      loss = loss_function(outputs[: , :, -1], targets.float())           # computes loss\n",
        "      optimiser.zero_grad()                                               # resets the gradients of all optimized tensors\n",
        "      loss.backward()                                                     # performs backward pass\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "      optimiser.step()                                                    # updates parameters\n",
        "      if (i+1) % 100 == 0:\n",
        "        print (f'Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "  print(\"Training complete\")"
      ],
      "metadata": {
        "id": "xAYhf_HqFIaF"
      },
      "id": "xAYhf_HqFIaF",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con el modelo configurado se procede al entrenamiento"
      ],
      "metadata": {
        "id": "Npy-phsPUp_X"
      },
      "id": "Npy-phsPUp_X"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "aa9c84ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa9c84ce",
        "outputId": "51888f8f-c4bd-4e2b-f53b-e1e9be63a2d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/640], Loss: 368257.7500\n",
            "Epoch [1/5], Step [200/640], Loss: 313254.0000\n",
            "Epoch [1/5], Step [300/640], Loss: 353371.7500\n",
            "Epoch [1/5], Step [400/640], Loss: 323731.2188\n",
            "Epoch [1/5], Step [500/640], Loss: 309512.5938\n",
            "Epoch [1/5], Step [600/640], Loss: 330767.4375\n",
            "Epoch [2/5], Step [100/640], Loss: 336133.1562\n",
            "Epoch [2/5], Step [200/640], Loss: 323307.1875\n",
            "Epoch [2/5], Step [300/640], Loss: 322032.9375\n",
            "Epoch [2/5], Step [400/640], Loss: 322483.7812\n",
            "Epoch [2/5], Step [500/640], Loss: 301092.5000\n",
            "Epoch [2/5], Step [600/640], Loss: 344871.7500\n",
            "Epoch [3/5], Step [100/640], Loss: 333856.3125\n",
            "Epoch [3/5], Step [200/640], Loss: 345922.8438\n",
            "Epoch [3/5], Step [300/640], Loss: 338726.3438\n",
            "Epoch [3/5], Step [400/640], Loss: 332800.4375\n",
            "Epoch [3/5], Step [500/640], Loss: 385547.9375\n",
            "Epoch [3/5], Step [600/640], Loss: 343711.1562\n",
            "Epoch [4/5], Step [100/640], Loss: 337052.7812\n",
            "Epoch [4/5], Step [200/640], Loss: 348363.6875\n",
            "Epoch [4/5], Step [300/640], Loss: 295095.2500\n",
            "Epoch [4/5], Step [400/640], Loss: 324688.3438\n",
            "Epoch [4/5], Step [500/640], Loss: 324288.0938\n",
            "Epoch [4/5], Step [600/640], Loss: 331931.7500\n",
            "Epoch [5/5], Step [100/640], Loss: 316447.8125\n",
            "Epoch [5/5], Step [200/640], Loss: 350188.6875\n",
            "Epoch [5/5], Step [300/640], Loss: 344032.8125\n",
            "Epoch [5/5], Step [400/640], Loss: 300971.0312\n",
            "Epoch [5/5], Step [500/640], Loss: 289565.2812\n",
            "Epoch [5/5], Step [600/640], Loss: 317780.0938\n",
            "Training complete\n"
          ]
        }
      ],
      "source": [
        "# Call the train function\n",
        "loss_function = nn.CrossEntropyLoss().to(device)  #Esta línea define la función de pérdida utilizada para evaluar el rendimiento del modelo durante el entrenamiento. La función de pérdida utilizada es nn.CrossEntropyLoss, que es una función de pérdida común para tareas de clasificación.\n",
        "lr = 0.0005                                       #establece la tasa de aprendizaje, que es un parámetro que controla la magnitud de las actualizaciones de parámetros durante el entrenamiento. Una tasa de aprendizaje más alta puede conducir a un aprendizaje más rápido, pero también puede aumentar la inestabilidad y el riesgo de sobreajuste.\n",
        "epochs = 5                                        #establece el número de épocas, que es el número de veces que se recorrerá todo el conjunto de datos de entrenamiento durante el proceso de aprendizaje. Un mayor número de épocas puede conducir a un mejor rendimiento, pero también puede aumentar el tiempo de entrenamiento.\n",
        "optimiser = optim.Adam(model.parameters(), lr=lr) #crea el optimizador, que es un algoritmo utilizado para actualizar los parámetros del modelo durante el entrenamiento. El optimizador utilizado es optim.Adam, que es un optimizador común para redes neuronales.\n",
        "train(model, epochs, optimiser)                   #llama a la función train para entrenar el modelo. La función train toma el modelo, el número de épocas y el optimizador como parámetros, y entrena el modelo utilizando el conjunto de datos de entrenamiento y la función de pérdida definida anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluacion del modelo generativo, como secuencia inicial se manda la frase \"I like\" y se restringe a 100 palabras totales."
      ],
      "metadata": {
        "id": "J5ibUh_6Vgxi"
      },
      "id": "J5ibUh_6Vgxi"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c8667411",
      "metadata": {
        "id": "c8667411"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_text, num_words, temperature=1.0):\n",
        "\n",
        "    model.eval()                            #Pone el modelo en modo de evaluación, lo que desactiva ciertas optimizaciones que solo son aplicables durante el entrenamiento.\n",
        "    words = tokeniser(start_text)           #Tokeniza el texto inicial utilizando el tokenizador proporcionado, dividiendo el texto en una lista de palabras individuales.\n",
        "    hidden = model.init_hidden(1)           #inicializa el estado oculto de la capa LSTM, que se utiliza para rastrear la memoria interna del modelo durante la generación de texto.\n",
        "    for i in range(0, num_words):\n",
        "        x = torch.tensor([[vocab[word] for word in words[i:]]], dtype=torch.long, device=device)  # crea un tensor que representa la secuencia de entrada para la iteración actual. El tensor selecciona las palabras de words comenzando en el índice i y las convierte a sus índices correspondientes en el vocabulario.\n",
        "        y_pred, hidden = model(x, hidden)                                                         #pasa el tensor de secuencia de entrada x y el estado oculto actual hidden al modelo. Devuelve los logaritmos de salida previstos y_pred y actualiza el estado oculto hidden.\n",
        "        last_word_logits = y_pred[0][-1]                                                          #extrae los logaritmos para la última palabra en la secuencia de salida prevista.\n",
        "        p = (F.softmax(last_word_logits / temperature, dim=0).detach()).to(device='cpu').numpy()  #aplica la activación softmax a los logaritmos, los convierte a probabilidades y transfiere la distribución de probabilidad resultante a la CPU.\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)                                 #selecciona un índice de palabra aleatorio de la distribución de probabilidad p.\n",
        "        words.append(vocab.lookup_token(word_index))                                              #agrega la palabra seleccionada a la lista de palabras generadas.\n",
        "\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Genera 60 palabras a partir del texto inicial \"I like\".\n",
        "print(generate_text(model, start_text=\"I like\", num_words=60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir_fxuMqWriK",
        "outputId": "e3b0dd1e-c7dd-4a79-ff74-68477738db56"
      },
      "id": "Ir_fxuMqWriK",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i like blossom kennedy harriet drumming 65th asserts corridors perón 1966 swim mausoleum splashes wading professional bodies convicts source molecule viola sakuraba jill beer colfer jfk seismic enforce widows crevice captained boletus enabling ethnic hinting larry among kelp morphology parsons monsen westbound mancuso venture factions suitability 1660 1207 seed interrupts brightly cadre dunn statute sachs kalyanasundara egg popularizing assumption imjin tennant fiancé\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusiones"
      ],
      "metadata": {
        "id": "M2VZcitoYDTW"
      },
      "id": "M2VZcitoYDTW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los modelos generativos son un tipo de modelo de aprendizaje automático que se utilizan para generar nuevas muestras de datos. Estos modelos son útiles para una variedad de tareas, como la creación de contenido creativo, la traducción de idiomas y la detección de fraudes.\n",
        "\n",
        "En el código anterior, se muestra cómo utilizar un modelo generativo LSTM para generar texto. El modelo se entrena en un conjunto de datos de texto, y luego se puede utilizar para generar nuevo texto. La función generate_text() del código toma un texto inicial como entrada y genera un número especificado de palabras.\n",
        "\n",
        "El código también muestra cómo entrenar un modelo generativo. La función train() del código toma el modelo, el número de épocas y el optimizador como parámetros. La función recorre el conjunto de datos de entrenamiento varias veces, actualizando los parámetros del modelo cada vez.\n",
        "\n",
        "Los modelos generativos tienen el potencial de revolucionar una variedad de industrias. En el ámbito del marketing, los modelos generativos se pueden utilizar para crear anuncios personalizados o contenido creativo. En el ámbito de la educación, los modelos generativos se pueden utilizar para crear materiales de aprendizaje personalizados o para ayudar a los estudiantes a escribir ensayos. En el ámbito de la atención médica, los modelos generativos se pueden utilizar para ayudar a los médicos a diagnosticar enfermedades o para desarrollar nuevos medicamentos.\n",
        "\n",
        "Sin embargo, los modelos generativos también presentan algunos desafíos. Un desafío es que los modelos generativos pueden generar texto que es sesgado o inexacto. Otro desafío es que los modelos generativos pueden ser utilizados para crear contenido dañino, como noticias falsas o propaganda.\n",
        "\n",
        "A pesar de estos desafíos, los modelos generativos tienen el potencial de ser una herramienta poderosa para el bien. Con el desarrollo de nuevas técnicas y la atención a los desafíos, los modelos generativos tienen el potencial de mejorar nuestras vidas de muchas maneras.\n",
        "\n",
        "Algunos ejemplos específicos de cómo se están utilizando los modelos generativos en la actualidad incluyen:\n",
        "\n",
        "En el ámbito de la educación, los modelos generativos se utilizan para ayudar a los estudiantes a aprender. Por ejemplo, la compañía Pearson utiliza un modelo generativo para crear ejercicios de práctica personalizados para los estudiantes.\n",
        "\n",
        "Estos son solo algunos ejemplos de cómo se están utilizando los modelos generativos en la actualidad. A medida que los modelos generativos continúen desarrollándose, es probable que veamos aún más casos de uso innovadores para esta tecnología."
      ],
      "metadata": {
        "id": "GxkuXGJIYF2Q"
      },
      "id": "GxkuXGJIYF2Q"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}